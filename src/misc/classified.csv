10.4018/978-1-7998-2543-2.ch004|'Fake News' in the context of information literacy: A Canadian case study|This chapter describes a study that interviewed 18participants (8professors, 6 librarians, and 4 department chairs) about their perceptions of 'fake news' in the context of their educational roles in information literacy (IL) within a large Canadian university. Qualitative analysis of the interviews reveals a substantial overlap in these educators ' perceptions of skills associated with IL and 'fake news ' detection. Librarians ' IL role seems to be undervalued. Better communication among integral IL educator groups is recommended. Most study participants emphasized the need for incorporating segments dedicated to detecting 'fake news' in IL curricula. Pro-active IL campaigns to prevent, detect, and deter the spread of various 'fakes' in digital media and specialized mis-/disinformation awareness courses are among best practices that support critical thinking and information evaluation within the societal context. Two other interventions, complementary to IL as per Rubin 's Disinformation and Misinformation Triangle, are suggested - detection automation technology and media regulation. © 2022 Elsevier B.V., All rights reserved.|Market

10.1109/ICAAIC56838.2023.10141355|Enhancing the Detection of Fake News in Social Media based on Machine Learning Models|Inaccurate or misleading reporting, sometimes known as 'fake news,' has become a significant problem in modern society. Fake news is a severe problem in the digital landscape, where anyone can publish their thoughts as though they were facts. However, research into what social media users take in is still in its infancy, and ongoing efforts aim to learn more about how social media users can identify false information. Discovering a reliable method for identifying fake news has been the biggest obstacle. Using supervised learning techniques that rely on labelled data to determine whether or not a piece of text is genuine helps us become aware of such stories. This study uses multiple methods, such as logistic regression, the XGBoost model, and the Random Forest model, to examine the characteristics of news articles and determine whether they are authentic. These models are typically trained on labelled data, with additional features derived from CNN-URL and Reddit data. The goal is to create more reliable sources of information and reduce the spread of misinformation by creating models that can accurately distinguish between real and fake news. © 2023 Elsevier B.V., All rights reserved.|Market

10.1109/IOTSMS59855.2023.10325722|Image to Text Recognition for Detecting Human and Machine Altered News in Social Media|Fake news on social media platforms has become a pressing issue, undermining the reliability of factual information. Platforms like Twitter, Instagram, and Reddit lack measures to recognize falsified information that is circulated via images. The existing state of the art solutions can ascertain the origin of a text string generated by a neural language model, they encounter difficulties when it comes to recognizing human-made modifications to news content and detecting instances of fake news conveyed through images. To address this challenge, we propose a solution that utilizes the Optical Character Recognition (OCR) capabilities of the Google Cloud Vision API to extract text from news images. The extracted text is then cross-referenced with the New York Times (NYT) database to verify the authenticity of the news articles. Our testing on human-altered, fake, and real news images yielded a 84.54% accuracy in detecting falsified news articles. This paper offers a promising approach to combat misinformation in image-based news content shared on social media platforms, thereby contributing to the preservation of factual information integrity. © 2023 Elsevier B.V., All rights reserved.|Market

10.2196/34218|Tracking Private WhatsApp Discourse about COVID-19 in Singapore: Longitudinal Infodemiology Study|Background: Worldwide, social media traffic increased following the onset of the COVID-19 pandemic. Although the spread of COVID-19 content has been described for several social media platforms (eg, Twitter and Facebook), little is known about how such content is spread via private messaging platforms, such as WhatsApp (WhatsApp LLC). Objective: In this study, we documented (1) how WhatsApp is used to transmit COVID-19 content, (2) the characteristics of WhatsApp users based on their usage patterns, and (3) how usage patterns link to COVID-19 concerns. Methods: We used the experience sampling method to track day-to-day WhatsApp usage during the COVID-19 pandemic. For 1 week, participants reported each day the extent to which they had received, forwarded, or discussed COVID-19 content. The final data set comprised 924 data points, which were collected from 151 participants. Results: During the weeklong monitoring process, most participants (143/151, 94.7%) reported at least 1 COVID-19-related use of WhatsApp. When a taxonomy was generated based on usage patterns, around 1 in 10 participants (21/151, 13.9%) were found to have received and shared a high volume of forwarded COVID-19 content, akin to super-spreaders identified on other social media platforms. Finally, those who engaged with more COVID-19 content in their personal chats were more likely to report having COVID-19-related thoughts throughout the day. Conclusions: Our findings provide a rare window into discourse on private messaging platforms. Such data can be used to inform risk communication strategies during the pandemic. © 2022 Elsevier B.V., All rights reserved.|Disease

10.1007/s00607-021-01013-w|MANIFESTO: a huMAN-centric explaInable approach for FakE news spreaders deTectiOn|Fake news spreading is strongly connected with the human involvement as individuals tend to fall, adopt and circulate misinformation stories. Until recently, the role of human characteristics in fake news diffusion, in order to deeply understand and fight misinformation patterns, has not been explored to the full extent. This paper suggests a human-centric approach on detecting fake news spreading behavior by building an explainable fake-news-spreader classifier based on psychological and behavioral cues of individuals. Our model achieves promising classification results while offering explanations of human motives and features behind fake news spreading behavior. Moreover, to the best of our knowledge, this is the first study that aims at providing a fully explainable setup that evaluates fake news spreading based on users credibility applied to public discussions aiming to a comprehensive way to combat fake news through human involvement. © 2022 Elsevier B.V., All rights reserved.|Unclear


10.4324/9780429295379-13|Your fake news, our facts: Identity-based motivation shapes what we believe, share, and accept|In making sense of experience and choosing a course of action, identities matter. People are more likely to accept and share messages that fit the way they make sense of themselves and their world. Messages that fit are more likely to stick and are less likely to be counterargued. One way to create this “fit” is to frame persuasion attempts in culturally fluent terms and yoke a call to action to the social categories people experience as “true” and “natural”. This two-step process (setting a culturally fluent frame and linking action to identity) shifts people from information-based to identity-based processing. Once this occurs, identities shape which facts matter; how much information is enough; how carefully information is scrutinized; and how much people accept, believe, and share rather than reject, disbelieve, and counterargue messages regarding these facts and information. We outline how this works, arguing that by combining cultural fluency and identities, disinformation may be more efficient than information or misinformation in rallying people to action and that corrective “undoing” attempts must address this culture-identity framing. © 2021 Elsevier B.V., All rights reserved.|War

10.1007/978-3-030-86967-0_15|A Comparative Study of Word Embeddings for the Construction of a Social Media Expert Filter|With the proliferation of fake news and misinformation on social media, being able to differentiate a reliable source of information has become increasingly important. In this paper we present a new algorithm for filtering expert users in social networks according to a certain topic under study. For the algorithm fine-tuning, a comparative study of results according to different word embeddings as well as different representation models, such as Skip-Gram and CBOW, is provided alongside the paper. © 2021 Elsevier B.V., All rights reserved.|Market

10.1109/ICIMTech50083.2020.9211124|Can the damage be undone? analyzing misinformation during COVID-19 outbreak in Indonesia|Indonesia is no stranger to misinformation, especially during the COVID-19 outbreak in early 2020. From 1 January 2020 to 15 April 2020, the Indonesian Ministry of Communication and Information was able to identify 534 misinformation circulating in the country. False information during a pandemic may pose a serious threat to public health. Therefore, this study aims to identify the type and claims of misinformation from all 534 articles. Also, based on the results, this study aims to give recommendations for dealing with misinformation in Indonesia. This research uses a qualitative approach through systematic analysis, where all articles were analyzed by three human coders based on a predefined coding scheme. Results show that the most common type is a reconfiguration from old or genuine materials compared to fully fabricated content. While the most common claims are concerning community spread and public authority action. Based on the results, this paper recommends reducing the number of misinformation through three perspectives - identification of misinformation, platforms, and support for targeted people. © 2020 Elsevier B.V., All rights reserved.|Disease

10.1145/3543507.3583388|Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation|The spread of online misinformation threatens public health, democracy, and the broader society. While professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. On the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation - recent research has shown that 96% counter-misinformation responses are made by ordinary users. However, research also found that 2/3 times, these responses are rude and lack evidence. This work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. This objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. In this work, we create two novel datasets of misinformation and counter-misinformation response pairs from in-the-wild social media and crowdsourcing from college-educated students. We annotate the collected data to distinguish poor from ideal responses that are factual, polite, and refute misinformation. We propose MisinfoCorrect, a reinforcement learning-based framework that learns to generate counter-misinformation responses for an input misinformation post. The model rewards the generator to increase the politeness, factuality, and refutation attitude while retaining text fluency and relevancy. Quantitative and qualitative evaluation shows that our model outperforms several baselines by generating high-quality counter-responses. This work illustrates the promise of generative text models for social good - here, to help create a safe and reliable information ecosystem. The code and data is accessible on https://github.com/claws-lab/MisinfoCorrect. © 2025 Elsevier B.V., All rights reserved.|War

10.1007/s11263-022-01606-8|Countering Malicious DeepFakes: Survey, Battleground, and Horizon|The creation or manipulation of facial appearance through deep generative approaches, known as DeepFake, have achieved significant progress and promoted a wide range of benign and malicious applications, e.g., visual effect assistance in movie and misinformation generation by faking famous persons. The evil side of this new technique poses another popular study, i.e., DeepFake detection aiming to identify the fake faces from the real ones. With the rapid development of the DeepFake-related studies in the community, both sides (i.e., DeepFake generation and detection) have formed the relationship of battleground, pushing the improvements of each other and inspiring new directions, e.g., the evasion of DeepFake detection. Nevertheless, the overview of such battleground and the new direction is unclear and neglected by recent surveys due to the rapid increase of related publications, limiting the in-depth understanding of the tendency and future works. To fill this gap, in this paper, we provide a comprehensive overview and detailed analysis of the research work on the topic of DeepFake generation, DeepFake detection as well as evasion of DeepFake detection, with more than 318 research papers carefully surveyed. We present the taxonomy of various DeepFake generation methods and the categorization of various DeepFake detection methods, and more importantly, we showcase the battleground between the two parties with detailed interactions between the adversaries (DeepFake generation) and the defenders (DeepFake detection). The battleground allows fresh perspective into the latest landscape of the DeepFake research and can provide valuable analysis towards the research challenges and opportunities as well as research trends and future directions. We also elaborately design interactive diagrams (http://www.xujuefei.com/dfsurvey) to allow researchers to explore their own interests on popular DeepFake generators or detectors. © 2022 Elsevier B.V., All rights reserved.|Unclear

10.1007/978-981-19-9307-7_7|Metadata Analysis of Web Images for Source Authentication in Online Social Media|Camera mobility-based embedded devices now facilitate easy creation, manipulation, and sharing of digital content on social media. While doing so, the shareable misinformation or disinformation content may harm society. Such contents may be variously forwarded without verified integrity and authenticity of the source on Online Social Networks (OSN). Thus, it is inevitable to trace the epicenter and the kind of information being spread on social networks. In this paper, we have investigated the types of metadata linked with digital images and analyzed the different attributes that are susceptible to squandering the integrity of source authentication with the easy availability of online tools and mobile-based apps. Finally, we accentuate protecting the metadata through the watermarking technique to reveal a piece of important information in the digital forensic investigation. © 2023 Elsevier B.V., All rights reserved.|Disease

10.1177/1329878X20951301|The challenges of responding to misinformation during a pandemic: content moderation and the limitations of the concept of harm|Social media have been central in informing people about the COVID-19 pandemic. They influence the ways in which information is perceived, communicated and shared online, especially with physical distancing measures in place. While these technologies have given people the opportunity to contribute to public discussions about COVID-19, the narratives disseminated on social media have also been characterised by uncertainty, disagreement, false and misleading advice. Global technology companies have responded to these concerns by introducing new content moderation policies based on the concept of harm to tackle the spread of misinformation and disinformation online. In this essay, we examine some of the key challenges in implementing these policies in real time and at scale, calling for more transparent and nuanced content moderation strategies to increase public trust and the quality of information about the pandemic consumed online. © 2020 Elsevier B.V., All rights reserved.|Market

10.1080/01296612.2020.1853393|Battling with infodemic and disinfodemic: the quandary of journalists to report on COVID-19 pandemic in Pakistan|The outbreak of COVID-19 pandemic has impacted all sectors of life. Despite economic downturn, one major impact of global pandemic is the rise of ‘infodemic’ and ‘disinfodemic’, which actually creates challenge for the public to access reliable information when they require it. News media plays a crucial role in such stressful situations by providing timely and accurate information about the pandemic. Nevertheless, when the news verification and gatekeeping is weak, dissemination of false information within the infodemic can result in the toxic disinfodemic of disinformation and misinformation. It is imperative to recognize that journalists, especially in restrained environments (like Pakistan), can combat infodemic and disinfodemic about the pandemic when their safety and accessibility to needed information are guaranteed, and when they are not prone to diverse challenges. Therefore, drawing on Reese’s hierarchy of influences model, this study seeks to explore the various levels of influences that impact on the Pakistani journalists’ reporting and their ability to deal with the challenges of infodemic and disinfodemic amid COVID-19 pandemic. Moreover, this study uses qualitative method of in-depth interviews (online) and employs thematic analysis to address the study’s findings. © 2021 Elsevier B.V., All rights reserved.|Mixed

10.2196/13512|Perceived patient-provider communication quality and sociodemographic factors associated with watching health-related videos on YouTube: A cross-sectional analysis|Background: Approximately 73% of US adults use YouTube, making it the most popular social media platform. Misinformation on social media is a growing concern; recent studies show a high proportion of misinformative health-related videos. Several studies on patient-provider communication and general health information seeking have been conducted. However, few studies to date have examined the potential association between patient-provider communication and health information seeking on specific social media platforms such as YouTube. A better understanding of this relationship may inform future health communication interventions. Objective: The aim was to use nationally representative cross-sectional data to describe the association between perceived patient-provider communication quality and sociodemographic factors on watching YouTube health-related videos. Methods: Data from the 2018 Health Information National Trends Survey were analyzed (N=3504). The primary outcome was whether participants watched a health-related video on YouTube over the past 12 months. A patient-provider communication composite score was created by summing responses about how often providers did the following: (1) gave you the chance to ask all the health-related questions you had, (2) gave attention to your feelings, (3) involved you in health care decisions as much as you wanted, (4) made sure that you understood the things you needed to do to take care of your health, (5) explained things in a way that you could understand, (6) spent enough time with you, and (7) helped you deal with feelings of uncertainty. Sociodemographic factors included age, gender, race/ethnicity, and education. Descriptive statistics and multivariable logistic regression were conducted. Results: Approximately 1067 (35% weighted prevalence) participants reported watching a health-related video on YouTube. Higher perceived quality of patient-provider communication on the composite score was significantly associated with lower odds of watching health-related videos on YouTube. Regarding sociodemographic factors, increasing age and being a high school graduate (compared with college graduate) were associated with lower odds of watching health-related videos on YouTube; whereas, Hispanic and non-Hispanic Asians were more likely to have watched a health-related video on YouTube. For individual aspects of patient-physician communication, two of seven patient-provider communication variables were significant. Those who reported that providers "sometimes" spent enough time with them had higher odds of watching a health-related video on YouTube, compared with those who said providers "always" spent enough time with them. Participants reporting that they "never" have a chance to ask all their health-related questions also had higher odds of watching health-related videos on YouTube compared with those who reported "always." Conclusions: Higher perceived quality of patient-provider communication is associated with lower odds of watching health-related videos on YouTube. When providers do not spend enough time or give an opportunity to ask questions, patients are more likely to pursue health information on social media. © 2020 Elsevier B.V., All rights reserved.|Market

10.1109/CogSIMA51574.2021.9475929|Challenges for Automatic Detection of Fake News Related to Migration : Invited paper|Fake news and misinformation is a widespread phenomenon these days, affecting social media, alternative and traditional media. In a climate of increasing polarization and perceived societal injustice, the topic of migration is one domain that is frequently the target of fake news, addressing both migrants and citizens in host countries. The problem is inherently a multi-lingual and multi-modal one in that it involves information in an array of languages, material in textual, visual and auditory form and often involves communication in a language which may be unfamiliar to recipients or which these recipients only may have basic knowledge of. We argue that semi-automatic approaches, empowering users to gain a clearer picture and base their decisions on sound information, are needed to counter the problem of misinformation. In order to deal with the scale of the problem, such approaches involve a variety of technologies from the field of Artificial Intelligence (AI). In this paper we identify a number of challenges related to implementing approaches for the detection of fake news in the context of migration. These include collecting multi-lingual and multi-modal datasets related to the migration domain, providing explanations of AI tools used in verification to both media professionals and consumers. Further efforts in truly collaborative AI will be needed. © 2021 Elsevier B.V., All rights reserved.|Market


10.1016/j.chb.2023.107971|Factors related to user perceptions of artificial intelligence (AI)-based content moderation on social media|Artificial intelligence (AI)-based moderation systems have been increasingly used by social media companies to identify and remove inappropriate user-generated content (e.g., misinformation) on their platforms. Previous research on AI moderation has primarily focused on situational and technological factors in predicting users’ perceptions of it, while little is known about the role of individual characteristics. To bridge this gap, this study examined whether and how familiarity, political ideology, and algorithm acceptance are related to perceptions of AI moderation. By analyzing survey data from a nationally representative panel in the United States (N = 4562), we found that individuals who were more familiar with AI moderation expressed less favorable perceptions of it. Those who identified themselves as liberals were more likely to view AI moderation positively than those who identified themselves as conservatives. The higher the algorithm acceptance, the more favorable the perception. Moreover, trust in AI moderation significantly mediated the relationship between these three individual characteristics (familiarity, political ideology, and algorithm acceptance) and perceptions. The findings enrich the current understanding of user responses to AI moderation and provide practical implications for policymakers and designers. © 2023 Elsevier B.V., All rights reserved.|Market

10.3390/vaccines11061054|Misinformation about the COVID-19 Vaccine in Online Catholic Media|During the COVID-19 pandemic, online media were the most widely used sources of scientific information. Often, they are also the only ones on science-related topics. Research has shown that much of the information available on the Internet about the health crisis lacked scientific rigor, and that misinformation about health issues can pose a threat to public health. In turn, millions of Catholics were found to be demonstrating against vaccination against COVID-19 based on “false” and misleading religious arguments. This research analyses publications about the vaccine in Catholic online media with the aim of understanding the presence of information (and misinformation) in this community. An algorithm designed for each media outlet collected COVID-19 vaccine-related publications from 109 Catholic media outlets in five languages. In total, 970 publications were analysed for journalistic genres, types of headlines and sources of information. The results show that most publications are informative and most of their headlines are neutral. However, opinion articles have mostly negative headlines. Furthermore, a higher percentage of the opinion authors come from the religious sphere and most of the sources cited are religious. Finally, 35% of the publications relate the vaccine to the framing issue of abortion. © 2023 Elsevier B.V., All rights reserved.|Unclear

10.1080/15424065.2021.1887787|Using Mobile Apps to Combat Fake News|Librarians and library professionals should challenge themselves to seek the most unbiased and truthful health news and information during a pandemic that has many patrons misled by fake news, disinformation, and misinformation. Through the use of mobile apps users can start to discern truth from lies, and users can learn how to more readily spot fake news. Mobile news apps also have advanced in their features, allowing users to share credible stories to stop the spread of fake news and disinformation and become part of the story by aiding in fact-checking, up-voting, and reporting false, misleading and biased news articles. © 2021 Elsevier B.V., All rights reserved.|