[{"id": "http://arxiv.org/abs/2306.08871v1", "title": "Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated\n  Misinformation in the Medical Domain", "contents": "Med-MMHL: A Multi-Modal Dataset for Detecting Human- and LLM-Generated Misinformation in the Medical Domain Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nTable 5: Statistics between diseases and news/tweets.\nInformation Type anemia arthritis asthma cancer covid diabetes epilepsy flu headache hypertension inflammation monkeypox parkinson pneumonia stroke Total\nReal news 64 85 148 1,410 859 332 48 740 70 55 282 44 81 50 286 4,554\nFake news 0 1 0 27 304 1 2 114 1 0 4 3 0 2 10 469\nLLM fake news 18 35 62 615 462 161 23 339 30 25 135 19 39 11 121 2,095\nTrue claims 3 4 7 190 1,619 31 2 362 11 1 12 7 4 9 21 2,283\nFalse claims 5 6 10 269 2,557 38 3 575 14 2 14 19 6 15 34 3,567\nTotal news 91 133 227 2,560 5,836 569 83 2,152 127 84 452 94 132 88 481 12,968\nReal tweets 53 15 28 540 1,161 152 29 2,095 36 21 174 8 35 17 106 7,738\nFake tweets 0 0 0 120 2,547 0 1 2,436 0 0 2 1,799 0 0 21 6,927\nTotal tweets 53 15 28 660 3,708 152 30 4,531 36 21 176 1,807 35 17 127 27,633\n(a) Fake News.\n (b) True News.\nFigure 6: Frequency of hashtags in tweets about fake and true news articles.\nThis ensures that the models trained on our dataset do not get\nmisled by features that are irrelevant to the content of the articles.\nA.4 Baseline Models\nThe following baseline fake news detection methods are considered\nfor medical misinformation detection:\n\u2022BERT [ 17]: A bi-directional transformer model pretrained on a\nlarge corpus of English data in a self-supervised fashion.\n\u2022BioBERT [ 16]: A sentence-transformers model built with medical\ndataset for fact-checking of online health information.\n\u2022Funnel Transformer [ 15]: An efficient bidirectional transformer\nmodel by applying a pooling operation after each layer, akin to\nconvolutional neural networks, to reduce the length of the input.\n\u2022FN-BERT [ 46]: A BERT-based model recently finetuned on a Fake\nnews classification dataset in 2023.\u2022sentenceBERT [ 41]: A sentence representation learning model\npretrained using Siamese and triplet network structures.\n\u2022distilBERT [ 40]: A dual-encoder then dot-product scoring ar-\nchitecture BERT model. The version employed in this paper is\npre-trained with the TAS-Balanced method on the MSMARCO\nstandard.\n\u2022dEFEND [ 42] utilizes the hierarchical attention network to model\narticle content for misinformation detection.\n\u2022CLIP [ 4]: A multi-modal vision and language model pretrained\non 400 million image-text pairs.\n\u2022VisualBERT [ 32]: A multi-modal vision and language model. It\nuses a BERT-like transformer to prepare embeddings for image-\ntext pairs."}, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null]